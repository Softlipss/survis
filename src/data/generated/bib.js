const generatedBibEntries = {
    "Ferenczi2023EthereumGas": {
        "abstract": "This study presents a probabilistic forecasting model for Ethereum gas prices using DeepAR. The model aims to capture the temporal dynamics of gas prices and provide accurate predictions, which are crucial for transaction cost estimation and network efficiency.",
        "author": "Andras Ferenczi and Costin B\u0103dic\u0103",
        "doi": "10.1080/24751839.2023.2250113",
        "journal": "Journal of Information and Telecommunication",
        "keywords": "Ethereum, gas price prediction, DeepAR, probabilistic forecasting, blockchain analytics",
        "number": "2",
        "pages": "1--15",
        "title": "Prediction of Ethereum Gas Prices Using DeepAR and Probabilistic Forecasting",
        "type": "article",
        "url": "https://doi.org/10.1080/24751839.2023.2250113",
        "volume": "8",
        "year": "2023"
    },
    "Gu2023Mamba": {
        "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
        "author": "Albert Gu and Tri Dao",
        "doi": "10.48550/arXiv.2312.00752",
        "journal": "arXiv preprint arXiv:2312.00752",
        "keywords": "sequence modeling, state space models, Mamba, deep learning",
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
        "type": "article",
        "url": "https://arxiv.org/abs/2312.00752",
        "year": "2023"
    },
    "Hong2023Comparative": {
        "abstract": "This study presents a comparative analysis focusing on Machine Learning (LightGBM), Deep Learning (LSTM), and Statistical (AR) methods for predicting narrow-based Exchange-Traded Funds (ETFs). The research evaluates the performance of these models in forecasting ETF market trends using multi-ticker data, aiming to identify the most effective approach for accurate and reliable predictions.",
        "author": "Mengze Hong and Zhiyuan Chen and Waleed Mahmoud Soliman and Kun Zhang",
        "booktitle": "Proceedings of the 6th International Conference on Machine Learning and Machine Intelligence (MLMI '23)",
        "doi": "10.1145/3635638.3635640",
        "keywords": "ETF prediction, machine learning, deep learning, time series forecasting, LightGBM, LSTM, autoregressive model",
        "pages": "1--7",
        "title": "A Comparative Study of LSTM, LightGBM, and Autoregressive Model in Narrow-Based ETF Market Prediction with Multi-Ticker Models",
        "type": "inproceedings",
        "url": "https://doi.org/10.1145/3635638.3635640",
        "year": "2023"
    },
    "Hong2023StockPrediction": {
        "abstract": "This study employs FinBERT for news sentiment analysis, combined with LSTM and GRU recurrent neural networks (RNN), aiming to elevate the accuracy of stock price predictions. By integrating sentiment scores into the RNN models, the research demonstrates improved forecasting performance, highlighting the significance of textual data in financial modeling.",
        "author": "Mengze Hong and Zhiyuan Chen and Waleed Mahmoud Soliman and Kun Zhang",
        "booktitle": "Proceedings of the 7th International Conference on Advanced Computing Systems (ICACS 2023)",
        "doi": "10.1145/3631908.3631919",
        "keywords": "stock prediction, FinBERT, sentiment analysis, LSTM, GRU, recurrent neural networks, financial forecasting",
        "pages": "1--7",
        "title": "Stock Price Prediction with FinBERT and RNN",
        "type": "inproceedings",
        "url": "https://doi.org/10.1145/3631908.3631919",
        "year": "2023"
    },
    "Li2024DeepARAttention": {
        "abstract": "We propose a novel DeepAR model based on the attention mechanism (DeepARA) for both single-point and probabilistic predictions of stock prices. This enhances the accuracy and flexibility of stock price forecasting.",
        "author": "Jiacheng Li and Wei Chen and Zhiheng Zhou and Junmei Yang",
        "doi": "10.1007/s00521-024-09916-3",
        "journal": "Neural Computing and Applications",
        "keywords": "stock price prediction, DeepAR, attention mechanism, probabilistic forecasting",
        "number": "25",
        "pages": "15389--15406",
        "title": "DeepAR-Attention Probabilistic Prediction for Stock Price Series",
        "type": "article",
        "url": "https://doi.org/10.1007/s00521-024-09916-3",
        "volume": "36",
        "year": "2024"
    },
    "Peng2024MoU": {
        "abstract": "Time series forecasting requires balancing short-term and long-term dependencies for accurate predictions. Existing methods mainly focus on long-term dependency modeling, neglecting the complexities of short-term dynamics, which may hinder performance. Transformers are superior in modeling long-term dependencies but are criticized for their quadratic computational cost. Mamba provides a near-linear alternative but is reported less effective in time series long-term forecasting due to potential information loss. Current architectures fall short in offering both high efficiency and strong performance for long-term dependency modeling. To address these challenges, we introduce Mixture of Universals (MoU), a versatile model to capture both short-term and long-term dependencies for enhancing performance in time series forecasting. MoU is composed of two novel designs: Mixture of Feature Extractors (MoF), an adaptive method designed to improve time series patch representations for short-term dependency, and Mixture of Architectures (MoA), which hierarchically integrates Mamba, FeedForward, Convolution, and Self-Attention architectures in a specialized order to model long-term dependency from a hybrid perspective. The proposed approach achieves state-of-the-art performance while maintaining relatively low computational costs. Extensive experiments on seven real-world datasets demonstrate the superiority of MoU.",
        "author": "Sijia Peng and Yun Xiong and Yangyong Zhu and Zhiqiang Shen",
        "doi": "10.48550/arXiv.2408.15997",
        "journal": "arXiv preprint arXiv:2408.15997",
        "keywords": "time series forecasting, Mamba, Transformer, Mixture of Universals, deep learning",
        "title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need",
        "type": "article",
        "url": "https://arxiv.org/abs/2408.15997",
        "year": "2024"
    },
    "Reis2025CovarianceForecasting": {
        "abstract": "Accurate covariance forecasting is central to portfolio allocation, risk management, and asset pricing, yet many existing methods struggle at medium-term horizons, where shifting market regimes and slower dynamics predominate. We propose a deep learning framework that combines three-dimensional convolutional neural networks, bidirectional long short-term memory layers, and multi-head attention to capture complex spatio-temporal dependencies. Using daily data on 14 exchange-traded funds from 2017 through 2023, we find that our model reduces Euclidean and Frobenius distance metrics by up to 20% relative to classical benchmarks (e.g., shrinkage and GARCH approaches) and remains robust across distinct market regimes. Our portfolio experiments demonstrate significant economic value through lower volatility and moderate turnover. These findings highlight the potential of advanced deep learning architectures to improve medium-term covariance forecasts, offering practical benefits for institutional investors and risk managers.",
        "author": "Pedro Reis and Ana Paula Serra and Jo\u00e3o Gama",
        "doi": "10.48550/arXiv.2503.01581",
        "journal": "arXiv preprint arXiv:2503.01581",
        "keywords": "covariance forecasting, deep learning, multi-asset portfolios, risk management",
        "title": "A Deep Learning Framework for Medium-Term Covariance Forecasting in Multi-Asset Portfolios",
        "type": "article",
        "url": "https://arxiv.org/abs/2503.01581",
        "year": "2025"
    },
    "Soliman2023ETF": {
        "abstract": "The significance of macroeconomic policy changes on ETF markets and financial markets cannot be disregarded. This study endeavors to predict the future trend of these markets by incorporating a group of selected economic indicators sourced from various ETF markets and utilizing probabilistic autoregressive recurrent networks (DeepAR). The choice of economic indicators was made based on the advice of a domain expert and the results of correlation estimation. These indicators were then divided into two categories: \"US\" indicators, which depict the impact of US policies such as the federal reserve fund rate and quantitative easing on the global markets, and \"region-specific\" indicators. The findings of the study indicate that the inclusion of \"US\" indicators enhances the prediction accuracy and that the DeepAR model outperforms the LSTM and GRU models. Furthermore, a web platform has been developed to apply the DeepAR models, which enables the user to predict the trend of an ETF ticker for the next 15 time-steps using the most recent data. The platform also possesses the capability to automatically generate fresh datasets from corresponding RESTful API sources in case the current data becomes outdated.",
        "author": "Soliman, Waleed Mahmoud and Chen, Zhiyuan and Johnson, Colin and Wong, Sabrina",
        "doi": "10.55549/epstem.1372067",
        "journal": "The Eurasia Proceedings of Science, Technology, Engineering \\& Mathematics",
        "keywords": "ETF markets, macroeconomic policy, DeepAR, time-series forecasting, financial prediction, machine learning",
        "pages": "485--494",
        "title": "ETF Markets' Prediction \\& Assets Management Platform Using Probabilistic Autoregressive Recurrent Networks",
        "type": "article",
        "url": "https://www.epstem.net/en/pub/issue/79793/1372067",
        "volume": "23",
        "year": "2023"
    },
    "Soliman2025ETF": {
        "abstract": "This study investigates whether incorporating sentiment analysis can enhance the accuracy of ETF price predictions. Specifically, we aim to predict ETF price movements using sentiment scores derived from news article summaries. Utilizing FinBERT for sentiment analysis, we quantify the sentiment of these summaries and integrate these scores into our predictive models. We employ DeepAR as a probabilistic model and compare its performance with LSTM in predicting ETF prices. The results demonstrate that DeepAR generally outperforms LSTM and that integrating sentiment scores significantly improves prediction accuracy. Given the promising outcomes, we also introduce a fixed \u201cSeed\u201d approach to ensure greater reliability and stability in our probabilistic predictions, addressing the need for robust sampling techniques in practical applications.",
        "author": "Waleed Soliman and Zhiyuan Chen and Colin Johnson and Sabrina Wong",
        "doi": "10.1002/isaf.70004",
        "journal": "Intelligent Systems in Accounting, Finance and Management",
        "keywords": "ETF prediction, sentiment analysis, DeepAR, FinBERT, time series forecasting, financial modeling",
        "number": "1",
        "pages": "1--15",
        "title": "Improving ETF Prediction Through Sentiment Analysis: A DeepAR and FinBERT Approach With Controlled Seed Sampling",
        "type": "article",
        "url": "https://doi.org/10.1002/isaf.70004",
        "volume": "32",
        "year": "2025"
    },
    "Wunsch2024UrbanWater": {
        "abstract": "The accurate and reliable short-term forecasting of urban water demand plays a crucial role in enabling drinking water utilities to operate sustainably and secure water supplies in the future. Here, we apply state-of-the-art DeepAR models to predict urban water demand in ten district metered areas (DMAs) in a water distribution system in northeastern Italy. DeepAR models are based on long short-term memory networks and can directly provide probabilistic results.",
        "author": "Andreas Wunsch and Christian K\u00fchnert and Steffen Wallner and Mathias Ziebarth",
        "booktitle": "Engineering Proceedings",
        "doi": "10.3390/engproc2024069025",
        "keywords": "urban water demand, DeepAR, time series forecasting, water utilities",
        "number": "1",
        "pages": "25",
        "title": "Urban Water Demand Forecasting Using DeepAR-Models as Part of the Battle of Water Demand Forecasting (BWDF)",
        "type": "inproceedings",
        "url": "https://doi.org/10.3390/engproc2024069025",
        "volume": "69",
        "year": "2024"
    }
};